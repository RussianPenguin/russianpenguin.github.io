---
layout: post
title: 'Kinect: о восстановлении координат и абберациях разного рода'
date: 2014-11-16 01:45:19.000000000 +03:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories:
- JFF
- kinect
tags:
- обработка изображений
meta:
  _wpcom_is_markdown: '1'
  sharing_disabled: '1'
  _wpas_skip_facebook: '1'
  _wpas_skip_google_plus: '1'
  _wpas_skip_twitter: '1'
  _wpas_skip_linkedin: '1'
  _wpas_skip_tumblr: '1'
  _wpas_skip_path: '1'
  _publicize_pending: '1'
  _rest_api_published: '1'
  _rest_api_client_id: "-1"
  _edit_last: '13696577'
author:
  login: russianpenguin
  email: maksim.v.zubkov@gmail.com
  display_name: russianpenguin
  first_name: Maksim
  last_name: Zubkov
permalink: "/2014/11/16/kinect-%d0%be-%d0%b2%d0%be%d1%81%d1%81%d1%82%d0%b0%d0%bd%d0%be%d0%b2%d0%bb%d0%b5%d0%bd%d0%b8%d0%b8-%d0%ba%d0%be%d0%be%d1%80%d0%b4%d0%b8%d0%bd%d0%b0%d1%82-%d0%b8-%d0%b0%d0%b1%d0%b1%d0%b5%d1%80%d0%b0/"
---
[![Вы не подумайте ничего - это плоскость.]({{ site.baseurl }}/assets/images/2014/11/d180d0b0d0b1d0bed187d0b5d0b5-d0bcd0b5d181d182d0be-2_116.png?w=300)](https://russianpenguin.files.wordpress.com/2014/11/d180d0b0d0b1d0bed187d0b5d0b5-d0bcd0b5d181d182d0be-2_116.png)Нет. Вы не подцмайте ничего. это плоскость - вид сбоку. А точнее мой потолок. И не посмотри я на него через сенсоры кинета, то в жизни бы не узнал, насколько он "плоский". :-D На самом деле потолок-то плоский. Но только разного рода нелинейные искажения, которые вносят сенсор и линзы не компенсируются ни встроенными калибровочными константами (которые зашиваются в каждую модель на заводе), ни функцией преобразования глубины кинекта в глубину реальную. Т.е. в метры/миллиметры.

Так что же не так?

Начнем с самого начала. Функция преобразования данных с датчика в глубину нелинейна по своей природе. Т.е. чем ближе объект наблюдения, чем с большей точностью мы можем мерять расстояние до объекта. Чем объект дальше, тем точность меньше. А это значит, что на 1 диницу шага датчика на ближнем расстоянии приходится меньше миллиметров.

На сейчас ребята из openni [предлагают](http://openkinect.org/wiki/Imaging_Information "Color\Depth mapping") новую версию функции преобразования сырых данных в метрические.

```
0.1236 \* tan(rawDisparity / 2842.5 + 1.1863)
```

Эта формула даст нам значения в метрах. Есть еще одна формула (она предлагалась раньше).

```
1/(rawDisparity \* -0.0030711016 + 3.3309495161)
```

Теперь говорят, что она морально устарела. :)

Посмотрим на обе формулы.

[![Функция вычисления реальной глубины изображения]({{ site.baseurl }}/assets/images/2014/11/d0b2d18bd0b4d0b5d0bbd0b5d0bdd0b8d0b5_120.png?w=300)](https://russianpenguin.files.wordpress.com/2014/11/d0b2d18bd0b4d0b5d0bbd0b5d0bdd0b8d0b5_120.png)Тут мы видим, что обе дают практически одинаковый результат. Даже если их и увеличить, то расхождения будут заметны только на сильно высоких значениях датчика. Стоит заметить, что по иксу - это сырые данные. А по игреку - восстановленное расстояние (в миллиметрах. для удобства обе функции были домножены на 1000).

Ок. Расстояние есть. Дальше нужно получать как-то координаты x и y. Так как с одной глубиной ничего интересного не получиться.

Тут все интереснее.

Мы знаем из спецификации сенсора (я говорю про первую версию если что):

- угол обзора по горизонтали - 58 градусов
- угол обзора по вертикали - 45 градусов
- количество точек с датчика - 640х480

И тут у нас должно появиться подозрение на эти спеки. Почему? Потому что ничего не говорится про относительный шаг между точками по горизонтали и по вертикали.

Как мы можем предположить - расстояние измеренное от плоскости отображения до вируальной камеры должно быть всегда одинаковое, как бы мы его не мерили.

Но что же получается. А получается следующее (изображение уперто [отсюда](http://stackoverflow.com/questions/17832238/kinect-intrinsic-parameters-from-field-of-view/18199938#18199938)).

[![Поиск расстояния]({{ site.baseurl }}/assets/images/2014/11/xk7pi.png)](https://russianpenguin.files.wordpress.com/2014/11/xk7pi.png)

```
tan(45/2)/tan(58/2) = 0,747261047
```

А это значит, что точка по вертикали будет 0,747261047, а точка по горизонтали - 1.

Отлично. соотношение расстояний между точками по горизонтали и вертикали нашли.

Тперь надо найти точки.

После того, как данные глубины получены с кинекта формируем массив координат.

```
(x\_v, y\_v. raw\_depth)
```

Здесь x\_v и y\_v - это всего лишь индексы строки и столба в двумерном массиве глубин.

```
x\_v = [0, 639] y\_v = [0, 479]
```

дальше надо лишь превратить эту точку в точку с реальными координатами.

```
z\_w = 123.6 \* tan(depth / 2842.5 + 1.1863)
```

Эм. А x\_w и y\_w? Тут все просто. По картинке выше мы знаем, что

```
x\_v/f = x\_w/z\_w
```

Где f - это расстояние от камеры, то вьюпорта.

```
f = MAX\_X/2 / tan(58/2) = 639/2 / tan(58/2)&nbsp; = 319.5/tan(58/2)
```

Или аналогично для оси ординат. Но тут не стоит забывать про коэффициент отношения между расстоянием по горизонтали и вертикали.

```
f = 0.747261047\*239.5/tan(45/2)
```

Теперь все просто

```
x\_w = (x\_v - 640/2) \* z\_w / f y\_w = (y\_v - 480/2) \* 0.747261047 \* z\_w / f
```

В случае с y\_w не забываем про масштабный коэффициент.

Но все эти вфводы позволяют нам лиш частично восстановить изображение. И все потому, что функция глубины - это фукция от трех переменных: x\_v, y\_v и depth, а не просто от depth. :) Так как влияние разного рода аббераций слишком велико (не то чтобы слишком +-10 самнтиметров по краям на растоянии 2 метра).

Поэтому пока мой потолок будет зображать из себя часть поверхности сферы (или цилиндра) неопределенного радиуса.

[![Потолок (или часть сферы?)]({{ site.baseurl }}/assets/images/2014/11/d180d0b0d0b1d0bed187d0b5d0b5-d0bcd0b5d181d182d0be-2_115.png)](https://russianpenguin.files.wordpress.com/2014/11/d180d0b0d0b1d0bed187d0b5d0b5-d0bcd0b5d181d182d0be-2_115.png)

**UPD**

[Код на гитхабе](https://github.com/RussianPenguin/kinectDepthView "Kinect depth view with Ogre3d")

