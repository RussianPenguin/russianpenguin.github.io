<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta name="yandex-verification" content="551ffc5ae1433df6" />
    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Kinect: о восстановлении координат и абберациях разного рода | Чтобы не забыть</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Kinect: о восстановлении координат и абберациях разного рода" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Нет. Вы не подцмайте ничего. это плоскость - вид сбоку. А точнее мой потолок. И не посмотри я на него через сенсоры кинета, то в жизни бы не узнал, насколько он “плоский”. :-D На самом деле потолок-то плоский. Но только разного рода нелинейные искажения, которые вносят сенсор и линзы не компенсируются ни встроенными калибровочными константами (которые зашиваются в каждую модель на заводе), ни функцией преобразования глубины кинекта в глубину реальную. Т.е. в метры/миллиметры. Так что же не так? Начнем с самого начала. Функция преобразования данных с датчика в глубину нелинейна по своей природе. Т.е. чем ближе объект наблюдения, чем с большей точностью мы можем мерять расстояние до объекта. Чем объект дальше, тем точность меньше. А это значит, что на 1 диницу шага датчика на ближнем расстоянии приходится меньше миллиметров. На сейчас ребята из openni предлагают новую версию функции преобразования сырых данных в метрические. 0.1236 \* tan(rawDisparity / 2842.5 + 1.1863) Эта формула даст нам значения в метрах. Есть еще одна формула (она предлагалась раньше). 1/(rawDisparity \* -0.0030711016 + 3.3309495161) Теперь говорят, что она морально устарела. :) Посмотрим на обе формулы. Тут мы видим, что обе дают практически одинаковый результат. Даже если их и увеличить, то расхождения будут заметны только на сильно высоких значениях датчика. Стоит заметить, что по иксу - это сырые данные. А по игреку - восстановленное расстояние (в миллиметрах. для удобства обе функции были домножены на 1000). Ок. Расстояние есть. Дальше нужно получать как-то координаты x и y. Так как с одной глубиной ничего интересного не получиться. Тут все интереснее. Мы знаем из спецификации сенсора (я говорю про первую версию если что): угол обзора по горизонтали - 58 градусов угол обзора по вертикали - 45 градусов количество точек с датчика - 640х480 И тут у нас должно появиться подозрение на эти спеки. Почему? Потому что ничего не говорится про относительный шаг между точками по горизонтали и по вертикали. Как мы можем предположить - расстояние измеренное от плоскости отображения до вируальной камеры должно быть всегда одинаковое, как бы мы его не мерили. Но что же получается. А получается следующее (изображение уперто отсюда). tan(45/2)/tan(58/2) = 0,747261047 А это значит, что точка по вертикали будет 0,747261047, а точка по горизонтали - 1. Отлично. соотношение расстояний между точками по горизонтали и вертикали нашли. Тперь надо найти точки. После того, как данные глубины получены с кинекта формируем массив координат. (x\_v, y\_v. raw\_depth) Здесь x_v и y_v - это всего лишь индексы строки и столба в двумерном массиве глубин. x\_v = [0, 639] y\_v = [0, 479] дальше надо лишь превратить эту точку в точку с реальными координатами. z\_w = 123.6 \* tan(depth / 2842.5 + 1.1863) Эм. А x_w и y_w? Тут все просто. По картинке выше мы знаем, что x\_v/f = x\_w/z\_w Где f - это расстояние от камеры, то вьюпорта. f = MAX\_X/2 / tan(58/2) = 639/2 / tan(58/2)&amp;nbsp; = 319.5/tan(58/2) Или аналогично для оси ординат. Но тут не стоит забывать про коэффициент отношения между расстоянием по горизонтали и вертикали. f = 0.747261047\*239.5/tan(45/2) Теперь все просто x\_w = (x\_v - 640/2) \* z\_w / f y\_w = (y\_v - 480/2) \* 0.747261047 \* z\_w / f В случае с y_w не забываем про масштабный коэффициент. Но все эти вфводы позволяют нам лиш частично восстановить изображение. И все потому, что функция глубины - это фукция от трех переменных: x_v, y_v и depth, а не просто от depth. :) Так как влияние разного рода аббераций слишком велико (не то чтобы слишком +-10 самнтиметров по краям на растоянии 2 метра). Поэтому пока мой потолок будет зображать из себя часть поверхности сферы (или цилиндра) неопределенного радиуса. UPD Код на гитхабе" />
<meta property="og:description" content="Нет. Вы не подцмайте ничего. это плоскость - вид сбоку. А точнее мой потолок. И не посмотри я на него через сенсоры кинета, то в жизни бы не узнал, насколько он “плоский”. :-D На самом деле потолок-то плоский. Но только разного рода нелинейные искажения, которые вносят сенсор и линзы не компенсируются ни встроенными калибровочными константами (которые зашиваются в каждую модель на заводе), ни функцией преобразования глубины кинекта в глубину реальную. Т.е. в метры/миллиметры. Так что же не так? Начнем с самого начала. Функция преобразования данных с датчика в глубину нелинейна по своей природе. Т.е. чем ближе объект наблюдения, чем с большей точностью мы можем мерять расстояние до объекта. Чем объект дальше, тем точность меньше. А это значит, что на 1 диницу шага датчика на ближнем расстоянии приходится меньше миллиметров. На сейчас ребята из openni предлагают новую версию функции преобразования сырых данных в метрические. 0.1236 \* tan(rawDisparity / 2842.5 + 1.1863) Эта формула даст нам значения в метрах. Есть еще одна формула (она предлагалась раньше). 1/(rawDisparity \* -0.0030711016 + 3.3309495161) Теперь говорят, что она морально устарела. :) Посмотрим на обе формулы. Тут мы видим, что обе дают практически одинаковый результат. Даже если их и увеличить, то расхождения будут заметны только на сильно высоких значениях датчика. Стоит заметить, что по иксу - это сырые данные. А по игреку - восстановленное расстояние (в миллиметрах. для удобства обе функции были домножены на 1000). Ок. Расстояние есть. Дальше нужно получать как-то координаты x и y. Так как с одной глубиной ничего интересного не получиться. Тут все интереснее. Мы знаем из спецификации сенсора (я говорю про первую версию если что): угол обзора по горизонтали - 58 градусов угол обзора по вертикали - 45 градусов количество точек с датчика - 640х480 И тут у нас должно появиться подозрение на эти спеки. Почему? Потому что ничего не говорится про относительный шаг между точками по горизонтали и по вертикали. Как мы можем предположить - расстояние измеренное от плоскости отображения до вируальной камеры должно быть всегда одинаковое, как бы мы его не мерили. Но что же получается. А получается следующее (изображение уперто отсюда). tan(45/2)/tan(58/2) = 0,747261047 А это значит, что точка по вертикали будет 0,747261047, а точка по горизонтали - 1. Отлично. соотношение расстояний между точками по горизонтали и вертикали нашли. Тперь надо найти точки. После того, как данные глубины получены с кинекта формируем массив координат. (x\_v, y\_v. raw\_depth) Здесь x_v и y_v - это всего лишь индексы строки и столба в двумерном массиве глубин. x\_v = [0, 639] y\_v = [0, 479] дальше надо лишь превратить эту точку в точку с реальными координатами. z\_w = 123.6 \* tan(depth / 2842.5 + 1.1863) Эм. А x_w и y_w? Тут все просто. По картинке выше мы знаем, что x\_v/f = x\_w/z\_w Где f - это расстояние от камеры, то вьюпорта. f = MAX\_X/2 / tan(58/2) = 639/2 / tan(58/2)&amp;nbsp; = 319.5/tan(58/2) Или аналогично для оси ординат. Но тут не стоит забывать про коэффициент отношения между расстоянием по горизонтали и вертикали. f = 0.747261047\*239.5/tan(45/2) Теперь все просто x\_w = (x\_v - 640/2) \* z\_w / f y\_w = (y\_v - 480/2) \* 0.747261047 \* z\_w / f В случае с y_w не забываем про масштабный коэффициент. Но все эти вфводы позволяют нам лиш частично восстановить изображение. И все потому, что функция глубины - это фукция от трех переменных: x_v, y_v и depth, а не просто от depth. :) Так как влияние разного рода аббераций слишком велико (не то чтобы слишком +-10 самнтиметров по краям на растоянии 2 метра). Поэтому пока мой потолок будет зображать из себя часть поверхности сферы (или цилиндра) неопределенного радиуса. UPD Код на гитхабе" />
<link rel="canonical" href="http://localhost:4000/2014/11/16/kinect-%D0%BE-%D0%B2%D0%BE%D1%81%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B8-%D0%BA%D0%BE%D0%BE%D1%80%D0%B4%D0%B8%D0%BD%D0%B0%D1%82-%D0%B8-%D0%B0%D0%B1%D0%B1%D0%B5%D1%80%D0%B0/" />
<meta property="og:url" content="http://localhost:4000/2014/11/16/kinect-%D0%BE-%D0%B2%D0%BE%D1%81%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B8-%D0%BA%D0%BE%D0%BE%D1%80%D0%B4%D0%B8%D0%BD%D0%B0%D1%82-%D0%B8-%D0%B0%D0%B1%D0%B1%D0%B5%D1%80%D0%B0/" />
<meta property="og:site_name" content="Чтобы не забыть" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2014-11-16T01:45:19+03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Kinect: о восстановлении координат и абберациях разного рода" />
<script type="application/ld+json">
{"headline":"Kinect: о восстановлении координат и абберациях разного рода","dateModified":"2014-11-16T01:45:19+03:00","datePublished":"2014-11-16T01:45:19+03:00","description":"Нет. Вы не подцмайте ничего. это плоскость - вид сбоку. А точнее мой потолок. И не посмотри я на него через сенсоры кинета, то в жизни бы не узнал, насколько он “плоский”. :-D На самом деле потолок-то плоский. Но только разного рода нелинейные искажения, которые вносят сенсор и линзы не компенсируются ни встроенными калибровочными константами (которые зашиваются в каждую модель на заводе), ни функцией преобразования глубины кинекта в глубину реальную. Т.е. в метры/миллиметры. Так что же не так? Начнем с самого начала. Функция преобразования данных с датчика в глубину нелинейна по своей природе. Т.е. чем ближе объект наблюдения, чем с большей точностью мы можем мерять расстояние до объекта. Чем объект дальше, тем точность меньше. А это значит, что на 1 диницу шага датчика на ближнем расстоянии приходится меньше миллиметров. На сейчас ребята из openni предлагают новую версию функции преобразования сырых данных в метрические. 0.1236 \\* tan(rawDisparity / 2842.5 + 1.1863) Эта формула даст нам значения в метрах. Есть еще одна формула (она предлагалась раньше). 1/(rawDisparity \\* -0.0030711016 + 3.3309495161) Теперь говорят, что она морально устарела. :) Посмотрим на обе формулы. Тут мы видим, что обе дают практически одинаковый результат. Даже если их и увеличить, то расхождения будут заметны только на сильно высоких значениях датчика. Стоит заметить, что по иксу - это сырые данные. А по игреку - восстановленное расстояние (в миллиметрах. для удобства обе функции были домножены на 1000). Ок. Расстояние есть. Дальше нужно получать как-то координаты x и y. Так как с одной глубиной ничего интересного не получиться. Тут все интереснее. Мы знаем из спецификации сенсора (я говорю про первую версию если что): угол обзора по горизонтали - 58 градусов угол обзора по вертикали - 45 градусов количество точек с датчика - 640х480 И тут у нас должно появиться подозрение на эти спеки. Почему? Потому что ничего не говорится про относительный шаг между точками по горизонтали и по вертикали. Как мы можем предположить - расстояние измеренное от плоскости отображения до вируальной камеры должно быть всегда одинаковое, как бы мы его не мерили. Но что же получается. А получается следующее (изображение уперто отсюда). tan(45/2)/tan(58/2) = 0,747261047 А это значит, что точка по вертикали будет 0,747261047, а точка по горизонтали - 1. Отлично. соотношение расстояний между точками по горизонтали и вертикали нашли. Тперь надо найти точки. После того, как данные глубины получены с кинекта формируем массив координат. (x\\_v, y\\_v. raw\\_depth) Здесь x_v и y_v - это всего лишь индексы строки и столба в двумерном массиве глубин. x\\_v = [0, 639] y\\_v = [0, 479] дальше надо лишь превратить эту точку в точку с реальными координатами. z\\_w = 123.6 \\* tan(depth / 2842.5 + 1.1863) Эм. А x_w и y_w? Тут все просто. По картинке выше мы знаем, что x\\_v/f = x\\_w/z\\_w Где f - это расстояние от камеры, то вьюпорта. f = MAX\\_X/2 / tan(58/2) = 639/2 / tan(58/2)&amp;nbsp; = 319.5/tan(58/2) Или аналогично для оси ординат. Но тут не стоит забывать про коэффициент отношения между расстоянием по горизонтали и вертикали. f = 0.747261047\\*239.5/tan(45/2) Теперь все просто x\\_w = (x\\_v - 640/2) \\* z\\_w / f y\\_w = (y\\_v - 480/2) \\* 0.747261047 \\* z\\_w / f В случае с y_w не забываем про масштабный коэффициент. Но все эти вфводы позволяют нам лиш частично восстановить изображение. И все потому, что функция глубины - это фукция от трех переменных: x_v, y_v и depth, а не просто от depth. :) Так как влияние разного рода аббераций слишком велико (не то чтобы слишком +-10 самнтиметров по краям на растоянии 2 метра). Поэтому пока мой потолок будет зображать из себя часть поверхности сферы (или цилиндра) неопределенного радиуса. UPD Код на гитхабе","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2014/11/16/kinect-%D0%BE-%D0%B2%D0%BE%D1%81%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B8-%D0%BA%D0%BE%D0%BE%D1%80%D0%B4%D0%B8%D0%BD%D0%B0%D1%82-%D0%B8-%D0%B0%D0%B1%D0%B1%D0%B5%D1%80%D0%B0/"},"@type":"BlogPosting","url":"http://localhost:4000/2014/11/16/kinect-%D0%BE-%D0%B2%D0%BE%D1%81%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B8-%D0%BA%D0%BE%D0%BE%D1%80%D0%B4%D0%B8%D0%BD%D0%B0%D1%82-%D0%B8-%D0%B0%D0%B1%D0%B1%D0%B5%D1%80%D0%B0/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=eacdde9998fcd3dd17b1956e7070eca69fdaf217">
  </head>
  <body>
    <header class="page-header" role="banner">
      <h1 class="project-name">Kinect: о восстановлении координат и абберациях разного рода</h1>
      <h2 class="project-tagline">Записная книжка рассеянного [в пространстве и времени] программиста</h2>
      
        <a href="https://github.com/RussianPenguin/russianpenguin.github.io" class="btn">View on GitHub</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1>Kinect: о восстановлении координат и абберациях разного рода</h1>
<p>16 Nov 2014</p>

<p><a href="https://russianpenguin.files.wordpress.com/2014/11/d180d0b0d0b1d0bed187d0b5d0b5-d0bcd0b5d181d182d0be-2_116.png"><img src="/assets/images/2014/11/d180d0b0d0b1d0bed187d0b5d0b5-d0bcd0b5d181d182d0be-2_116.png?w=300" alt="Вы не подумайте ничего - это плоскость." /></a>Нет. Вы не подцмайте ничего. это плоскость - вид сбоку. А точнее мой потолок. И не посмотри я на него через сенсоры кинета, то в жизни бы не узнал, насколько он “плоский”. :-D На самом деле потолок-то плоский. Но только разного рода нелинейные искажения, которые вносят сенсор и линзы не компенсируются ни встроенными калибровочными константами (которые зашиваются в каждую модель на заводе), ни функцией преобразования глубины кинекта в глубину реальную. Т.е. в метры/миллиметры.</p>

<p>Так что же не так?</p>

<p>Начнем с самого начала. Функция преобразования данных с датчика в глубину нелинейна по своей природе. Т.е. чем ближе объект наблюдения, чем с большей точностью мы можем мерять расстояние до объекта. Чем объект дальше, тем точность меньше. А это значит, что на 1 диницу шага датчика на ближнем расстоянии приходится меньше миллиметров.</p>

<p>На сейчас ребята из openni <a href="http://openkinect.org/wiki/Imaging_Information" title="Color\Depth mapping">предлагают</a> новую версию функции преобразования сырых данных в метрические.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.1236 \* tan(rawDisparity / 2842.5 + 1.1863)
</code></pre></div></div>

<p>Эта формула даст нам значения в метрах. Есть еще одна формула (она предлагалась раньше).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1/(rawDisparity \* -0.0030711016 + 3.3309495161)
</code></pre></div></div>

<p>Теперь говорят, что она морально устарела. :)</p>

<p>Посмотрим на обе формулы.</p>

<p><a href="https://russianpenguin.files.wordpress.com/2014/11/d0b2d18bd0b4d0b5d0bbd0b5d0bdd0b8d0b5_120.png"><img src="/assets/images/2014/11/d0b2d18bd0b4d0b5d0bbd0b5d0bdd0b8d0b5_120.png?w=300" alt="Функция вычисления реальной глубины изображения" /></a>Тут мы видим, что обе дают практически одинаковый результат. Даже если их и увеличить, то расхождения будут заметны только на сильно высоких значениях датчика. Стоит заметить, что по иксу - это сырые данные. А по игреку - восстановленное расстояние (в миллиметрах. для удобства обе функции были домножены на 1000).</p>

<p>Ок. Расстояние есть. Дальше нужно получать как-то координаты x и y. Так как с одной глубиной ничего интересного не получиться.</p>

<p>Тут все интереснее.</p>

<p>Мы знаем из спецификации сенсора (я говорю про первую версию если что):</p>

<ul>
  <li>угол обзора по горизонтали - 58 градусов</li>
  <li>угол обзора по вертикали - 45 градусов</li>
  <li>количество точек с датчика - 640х480</li>
</ul>

<p>И тут у нас должно появиться подозрение на эти спеки. Почему? Потому что ничего не говорится про относительный шаг между точками по горизонтали и по вертикали.</p>

<p>Как мы можем предположить - расстояние измеренное от плоскости отображения до вируальной камеры должно быть всегда одинаковое, как бы мы его не мерили.</p>

<p>Но что же получается. А получается следующее (изображение уперто <a href="http://stackoverflow.com/questions/17832238/kinect-intrinsic-parameters-from-field-of-view/18199938#18199938">отсюда</a>).</p>

<p><a href="https://russianpenguin.files.wordpress.com/2014/11/xk7pi.png"><img src="/assets/images/2014/11/xk7pi.png" alt="Поиск расстояния" /></a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tan(45/2)/tan(58/2) = 0,747261047
</code></pre></div></div>

<p>А это значит, что точка по вертикали будет 0,747261047, а точка по горизонтали - 1.</p>

<p>Отлично. соотношение расстояний между точками по горизонтали и вертикали нашли.</p>

<p>Тперь надо найти точки.</p>

<p>После того, как данные глубины получены с кинекта формируем массив координат.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(x\_v, y\_v. raw\_depth)
</code></pre></div></div>

<p>Здесь x_v и y_v - это всего лишь индексы строки и столба в двумерном массиве глубин.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x\_v = [0, 639] y\_v = [0, 479]
</code></pre></div></div>

<p>дальше надо лишь превратить эту точку в точку с реальными координатами.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>z\_w = 123.6 \* tan(depth / 2842.5 + 1.1863)
</code></pre></div></div>

<p>Эм. А x_w и y_w? Тут все просто. По картинке выше мы знаем, что</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x\_v/f = x\_w/z\_w
</code></pre></div></div>

<p>Где f - это расстояние от камеры, то вьюпорта.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>f = MAX\_X/2 / tan(58/2) = 639/2 / tan(58/2)&amp;nbsp; = 319.5/tan(58/2)
</code></pre></div></div>

<p>Или аналогично для оси ординат. Но тут не стоит забывать про коэффициент отношения между расстоянием по горизонтали и вертикали.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>f = 0.747261047\*239.5/tan(45/2)
</code></pre></div></div>

<p>Теперь все просто</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x\_w = (x\_v - 640/2) \* z\_w / f y\_w = (y\_v - 480/2) \* 0.747261047 \* z\_w / f
</code></pre></div></div>

<p>В случае с y_w не забываем про масштабный коэффициент.</p>

<p>Но все эти вфводы позволяют нам лиш частично восстановить изображение. И все потому, что функция глубины - это фукция от трех переменных: x_v, y_v и depth, а не просто от depth. :) Так как влияние разного рода аббераций слишком велико (не то чтобы слишком +-10 самнтиметров по краям на растоянии 2 метра).</p>

<p>Поэтому пока мой потолок будет зображать из себя часть поверхности сферы (или цилиндра) неопределенного радиуса.</p>

<p><a href="https://russianpenguin.files.wordpress.com/2014/11/d180d0b0d0b1d0bed187d0b5d0b5-d0bcd0b5d181d182d0be-2_115.png"><img src="/assets/images/2014/11/d180d0b0d0b1d0bed187d0b5d0b5-d0bcd0b5d181d182d0be-2_115.png" alt="Потолок (или часть сферы?)" /></a></p>

<p><strong>UPD</strong></p>

<p><a href="https://github.com/RussianPenguin/kinectDepthView" title="Kinect depth view with Ogre3d">Код на гитхабе</a></p>




      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/RussianPenguin/russianpenguin.github.io">russianpenguin.github.io</a> is maintained by <a href="https://github.com/RussianPenguin">RussianPenguin</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
   (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
   m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
   (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

   ym(69889387, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
   });
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/69889387" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
<!-- /Yandex.Metrika counter -->
  </body>
</html>
